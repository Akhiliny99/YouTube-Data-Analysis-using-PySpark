{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff5806af-168f-4542-a8ee-937c99a12832",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Extractor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c285a2-6ea1-4424-b507-05a07f5e469d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Define get_data_source function\n",
    "def get_data_source(data_type, file_path):\n",
    "    spark = SparkSession.builder.appName(\"Extractor\").getOrCreate()\n",
    "    if data_type == \"csv\":\n",
    "        return spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "    elif data_type == \"delta\":\n",
    "        return spark.read.format(\"delta\").table(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported data type\")\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AirpodsAfterIphone(Extractor):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self):\n",
    "        # Using the get_data_source function to load data\n",
    "        transaction_input_df = get_data_source(\n",
    "            data_type=\"csv\",\n",
    "            file_path=\"dbfs:/FileStore/tables/Transaction_Updated.csv\"\n",
    "        )\n",
    "\n",
    "        customer_input_df = get_data_source(\n",
    "            data_type=\"delta\",\n",
    "            file_path=\"default.customer_delta_table\"\n",
    "        )\n",
    "\n",
    "        return transaction_input_df, customer_input_df\n",
    "\n",
    "\n",
    "# Run the extractor\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the extractor and extract data\n",
    "    extractor = AirpodsAfterIphone()\n",
    "    try:\n",
    "        transaction_input_df, customer_input_df = extractor.extract()\n",
    "\n",
    "        # Check if data is extracted properly\n",
    "        if transaction_input_df is not None and customer_input_df is not None:\n",
    "            transaction_input_df.show()\n",
    "            customer_input_df.show()\n",
    "        else:\n",
    "            print(\"❌ No data extracted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0273e2-b4d4-49fb-b1c6-bb49a619c2be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Customer data loaded successfully.\nDisplaying transaction_input_df:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying customer_input_df:\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n🔍 Filtering: Customers who bought an iPhone and then AirPods\n+--------------+-----------+------------+----------------+-----------------+-------------+-------------------+--------+\n|transaction_id|customer_id|product_name|transaction_date|next_product_name|customer_name|          join_date|location|\n+--------------+-----------+------------+----------------+-----------------+-------------+-------------------+--------+\n|            11|        105|      iPhone|      2022-02-01|          AirPods|          Eva|2022-01-01 00:00:00|    Ohio|\n|            15|        108|      iPhone|      2022-02-05|          AirPods|        Henry|2022-04-01 00:00:00|    Utah|\n+--------------+-----------+------------+----------------+-----------------+-------------+-------------------+--------+\n\n✅ Final Processed Data:\nOut[5]: DataFrame[transaction_id: int, customer_id: string, product_name: string, transaction_date: date, next_product_name: string, customer_name: string, join_date: timestamp, location: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, col, broadcast\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Customer data loaded successfully.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        # ✅ Call Transformer after loading data\n",
    "        transformer = FirstTransformer()\n",
    "        transformedDF = transformer.transform(input_dfs)\n",
    "\n",
    "        return transformedDF  # Ensure transformed data is returned\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def transform(self, inputDFs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self, inputDFs):\n",
    "        transaction_input_df = inputDFs.get(\"transaction_input_df\")\n",
    "        customer_input_df = inputDFs.get(\"customer_input_df\")  # Ensure correct key name\n",
    "\n",
    "        if transaction_input_df is None or customer_input_df is None:\n",
    "            print(\"❌ Error: Required DataFrames not found in inputDFs\")\n",
    "            return None\n",
    "\n",
    "        print(\"Displaying transaction_input_df:\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        print(\"Displaying customer_input_df:\")\n",
    "        customer_input_df.show()\n",
    "\n",
    "        # ✅ Convert `customer_id` in transaction_input_df to match customer_input_df type (string)\n",
    "        transaction_input_df = transaction_input_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "\n",
    "        # Define Window Specification\n",
    "        windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "        # Apply lead function to get next product\n",
    "        transformedDF = transaction_input_df.withColumn(\n",
    "            \"next_product_name\", lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "\n",
    "        print(\"🔍 Filtering: Customers who bought an iPhone and then AirPods\")\n",
    "        filteredDF = transformedDF.filter(\n",
    "            (transformedDF.product_name == \"iPhone\") & \n",
    "            (transformedDF.next_product_name == \"AirPods\")\n",
    "        )\n",
    "\n",
    "        # ✅ Apply Broadcast Join for better performance and include missing columns\n",
    "        finalDF = filteredDF.join(broadcast(customer_input_df), \"customer_id\", \"inner\") \\\n",
    "                            .select(\"transaction_id\", \"customer_id\", \"product_name\", \n",
    "                                    \"transaction_date\", \"next_product_name\",\n",
    "                                    \"customer_name\", \"join_date\", \"location\")\n",
    "\n",
    "        # Show results\n",
    "        finalDF.show()\n",
    "\n",
    "        print(\"✅ Final Processed Data:\")\n",
    "        return finalDF  # Returning cleaned DataFrame\n",
    "\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7ea8e3-b9c4-40c0-ac3b-1a5e7d56640f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Customer data loaded successfully.\nDisplaying transaction_input_df:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying customer_input_df:\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n🔍 Filtering: Customers who bought an iPhone and then AirPods\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        105|          Eva|    Ohio|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\n✅ Final Processed Data (JOINDF):\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        105|          Eva|    Ohio|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\nOut[3]: DataFrame[customer_id: string, customer_name: string, location: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, col\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Customer data loaded successfully.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        # ✅ Call Transformer after loading data\n",
    "        transformer = FirstTransformer()\n",
    "        transformedDF = transformer.transform(input_dfs)\n",
    "\n",
    "        return transformedDF  # Ensure transformed data is returned\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def transform(self, inputDFs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self, inputDFs):\n",
    "        transaction_input_df = inputDFs.get(\"transaction_input_df\")\n",
    "        customer_input_df = inputDFs.get(\"customer_input_df\")  # Ensure correct key name\n",
    "\n",
    "        if transaction_input_df is None or customer_input_df is None:\n",
    "            print(\"❌ Error: Required DataFrames not found in inputDFs\")\n",
    "            return None\n",
    "\n",
    "        print(\"Displaying transaction_input_df:\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        print(\"Displaying customer_input_df:\")\n",
    "        customer_input_df.show()\n",
    "\n",
    "        # ✅ Convert `customer_id` in transaction_input_df to match customer_input_df type (string)\n",
    "        transaction_input_df = transaction_input_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "\n",
    "        # Define Window Specification\n",
    "        windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "        # Apply lead function to get next product\n",
    "        transformedDF = transaction_input_df.withColumn(\n",
    "            \"next_product_name\", lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "\n",
    "        print(\"🔍 Filtering: Customers who bought an iPhone and then AirPods\")\n",
    "        filteredDF = transformedDF.filter(\n",
    "            (transformedDF.product_name == \"iPhone\") & \n",
    "            (transformedDF.next_product_name == \"AirPods\")\n",
    "        )\n",
    "\n",
    "        # **Join with customer data on customer_id**\n",
    "        finalDF = filteredDF.join(customer_input_df, \"customer_id\", \"inner\") \\\n",
    "                            .select(\"customer_id\", \"customer_name\", \"location\")\n",
    "\n",
    "        # Show results\n",
    "        finalDF.show()\n",
    "\n",
    "        # ✅ Fix duplicate columns issue\n",
    "        joinDF = finalDF.join(customer_input_df, \"customer_id\").select(\n",
    "            finalDF[\"customer_id\"], finalDF[\"customer_name\"],finalDF[\"location\"]\n",
    "        )\n",
    "\n",
    "        print(\"✅ Final Processed Data (JOINDF):\")\n",
    "        joinDF.show()\n",
    "\n",
    "        return joinDF  # Returning cleaned DataFrame\n",
    "\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b0829f1-c8ea-4c0c-96f4-ba7f8738efdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Transformation complete.\nDisplaying transaction_input_df:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying customer_input_df:\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\nFiltering: Customers who bought an iPhone and then AirPods\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\nJOINDF\n+-----------+-------------+-------------------+--------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|          Eva|2022-01-01 00:00:00|    Ohio|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+-------------+-------------------+--------+\n\nOut[20]: DataFrame[customer_id: string, customer_name: string, join_date: timestamp, location: string, customer_name: string, join_date: timestamp, location: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, col\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        # ✅ Call Transformer after loading data\n",
    "        transformer = FirstTransformer()\n",
    "        transformedDF = transformer.transform(input_dfs)\n",
    "\n",
    "        return transformedDF  # Ensure transformed data is returned if needed\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def transform(self, inputDFs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self, inputDFs):\n",
    "        transaction_input_df = inputDFs.get(\"transaction_input_df\")\n",
    "        customer_input_df = inputDFs.get(\"customer_input_df\")  # Ensure correct key name\n",
    "\n",
    "        if transaction_input_df is None or customer_input_df is None:\n",
    "            print(\"Error: Required DataFrames not found in inputDFs\")\n",
    "            return None\n",
    "\n",
    "        print(\"Displaying transaction_input_df:\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        print(\"Displaying customer_input_df:\")\n",
    "        customer_input_df.show()\n",
    "\n",
    "        # ✅ Convert `customer_id` in transaction_input_df to match customer_input_df type (string)\n",
    "        transaction_input_df = transaction_input_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "\n",
    "        # Define Window Specification\n",
    "        windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "        # Apply lead function to get next product\n",
    "        transformedDF = transaction_input_df.withColumn(\n",
    "            \"next_product_name\", lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "\n",
    "        print(\"Filtering: Customers who bought an iPhone and then AirPods\")\n",
    "        filteredDF = transformedDF.filter(\n",
    "            (transformedDF.product_name == \"iPhone\") & \n",
    "            (transformedDF.next_product_name == \"AirPods\")\n",
    "        )\n",
    "\n",
    "        # **Join with customer data on customer_id**\n",
    "        finalDF = filteredDF.join(customer_input_df, \"customer_id\", \"inner\") \\\n",
    "                            .select(\"customer_id\", \"customer_name\", \"join_date\", \"location\")\n",
    "\n",
    "        # Show results\n",
    "        finalDF.show()\n",
    "\n",
    "        # Additional Join Logic (if required)\n",
    "        joinDF = finalDF.join(customer_input_df, \"customer_id\")  # Joining finalDF with customer data again (if needed)\n",
    "        print(\"JOINDF\")\n",
    "        joinDF.show()\n",
    "\n",
    "        return joinDF  # Returning joinDF instead of finalDF if required\n",
    "\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bdaa16a-ab86-4c9d-9b3c-dc461e58f7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Customer data loaded successfully.\nDisplaying transaction_input_df:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying customer_input_df:\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n🔍 Filtering: Customers who bought an iPhone and then AirPods\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Final Processed Data (JOINDF):\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\nOut[21]: DataFrame[customer_id: string, customer_name: string, join_date: timestamp, location: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, col\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Customer data loaded successfully.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        # ✅ Call Transformer after loading data\n",
    "        transformer = FirstTransformer()\n",
    "        transformedDF = transformer.transform(input_dfs)\n",
    "\n",
    "        return transformedDF  # Ensure transformed data is returned\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def transform(self, inputDFs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self, inputDFs):\n",
    "        transaction_input_df = inputDFs.get(\"transaction_input_df\")\n",
    "        customer_input_df = inputDFs.get(\"customer_input_df\")  # Ensure correct key name\n",
    "\n",
    "        if transaction_input_df is None or customer_input_df is None:\n",
    "            print(\"❌ Error: Required DataFrames not found in inputDFs\")\n",
    "            return None\n",
    "\n",
    "        print(\"Displaying transaction_input_df:\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        print(\"Displaying customer_input_df:\")\n",
    "        customer_input_df.show()\n",
    "\n",
    "        # ✅ Convert `customer_id` in transaction_input_df to match customer_input_df type (string)\n",
    "        transaction_input_df = transaction_input_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "\n",
    "        # Define Window Specification\n",
    "        windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "        # Apply lead function to get next product\n",
    "        transformedDF = transaction_input_df.withColumn(\n",
    "            \"next_product_name\", lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "\n",
    "        print(\"🔍 Filtering: Customers who bought an iPhone and then AirPods\")\n",
    "        filteredDF = transformedDF.filter(\n",
    "            (transformedDF.product_name == \"iPhone\") & \n",
    "            (transformedDF.next_product_name == \"AirPods\")\n",
    "        )\n",
    "\n",
    "        # **Join with customer data on customer_id**\n",
    "        finalDF = filteredDF.join(customer_input_df, \"customer_id\", \"inner\") \\\n",
    "                            .select(\"customer_id\", \"customer_name\", \"join_date\", \"location\")\n",
    "\n",
    "        # Show results\n",
    "        finalDF.show()\n",
    "\n",
    "        # ✅ Fix duplicate columns issue\n",
    "        joinDF = finalDF.join(customer_input_df, \"customer_id\").select(\n",
    "            finalDF[\"customer_id\"], finalDF[\"customer_name\"], finalDF[\"join_date\"], finalDF[\"location\"]\n",
    "        )\n",
    "\n",
    "        print(\"✅ Final Processed Data (JOINDF):\")\n",
    "        joinDF.show()\n",
    "\n",
    "        return joinDF  # Returning cleaned DataFrame\n",
    "\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab5adcd-3df2-41bc-a40b-d8aad92f5d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Customer data loaded successfully.\nDisplaying transaction_input_df:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying customer_input_df:\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n🔍 Filtering: Customers who bought an iPhone and then AirPods\n+-----------+--------------+------------+----------------+-----------------+-------------+-------------------+--------+\n|customer_id|transaction_id|product_name|transaction_date|next_product_name|customer_name|          join_date|location|\n+-----------+--------------+------------+----------------+-----------------+-------------+-------------------+--------+\n|        105|            11|      iPhone|      2022-02-01|          AirPods|          Eva|2022-01-01 00:00:00|    Ohio|\n|        108|            15|      iPhone|      2022-02-05|          AirPods|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+--------------+------------+----------------+-----------------+-------------+-------------------+--------+\n\n✅ Final Processed Data:\nOut[22]: DataFrame[customer_id: string, transaction_id: int, product_name: string, transaction_date: date, next_product_name: string, customer_name: string, join_date: timestamp, location: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, col\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Customer data loaded successfully.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        # ✅ Call Transformer after loading data\n",
    "        transformer = FirstTransformer()\n",
    "        transformedDF = transformer.transform(input_dfs)\n",
    "\n",
    "        return transformedDF  # Ensure transformed data is returned\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def transform(self, inputDFs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self, inputDFs):\n",
    "        transaction_input_df = inputDFs.get(\"transaction_input_df\")\n",
    "        customer_input_df = inputDFs.get(\"customer_input_df\")  # Ensure correct key name\n",
    "\n",
    "        if transaction_input_df is None or customer_input_df is None:\n",
    "            print(\"❌ Error: Required DataFrames not found in inputDFs\")\n",
    "            return None\n",
    "\n",
    "        print(\"Displaying transaction_input_df:\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        print(\"Displaying customer_input_df:\")\n",
    "        customer_input_df.show()\n",
    "\n",
    "        # ✅ Convert `customer_id` in transaction_input_df to match customer_input_df type (string)\n",
    "        transaction_input_df = transaction_input_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "\n",
    "        # Define Window Specification\n",
    "        windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "        # Apply lead function to get next product\n",
    "        transformedDF = transaction_input_df.withColumn(\n",
    "            \"next_product_name\", lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "\n",
    "        print(\"🔍 Filtering: Customers who bought an iPhone and then AirPods\")\n",
    "        filteredDF = transformedDF.filter(\n",
    "            (transformedDF.product_name == \"iPhone\") & \n",
    "            (transformedDF.next_product_name == \"AirPods\")\n",
    "        )\n",
    "\n",
    "        # ✅ Automatically join and include all columns from both DataFrames\n",
    "        finalDF = filteredDF.join(customer_input_df, \"customer_id\", \"inner\")\n",
    "\n",
    "        # Show results\n",
    "        finalDF.show()\n",
    "\n",
    "        print(\"✅ Final Processed Data:\")\n",
    "        return finalDF  # Returning cleaned DataFrame\n",
    "\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43492dfd-3744-4f3b-ba03-1e71225a3d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Customer data loaded successfully.\nDisplaying transaction_input_df:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying customer_input_df:\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n🔍 Filtering: Customers who bought an iPhone and then AirPods\n+-----------+-------------+--------+\n|customer_id|customer_name|location|\n+-----------+-------------+--------+\n|        105|          Eva|    Ohio|\n|        108|        Henry|    Utah|\n+-----------+-------------+--------+\n\n✅ Final Processed Data:\nOut[23]: DataFrame[customer_id: string, customer_name: string, location: string]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead, col\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Customer data loaded successfully.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        # ✅ Call Transformer after loading data\n",
    "        transformer = FirstTransformer()\n",
    "        transformedDF = transformer.transform(input_dfs)\n",
    "\n",
    "        return transformedDF  # Ensure transformed data is returned\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "    def transform(self, inputDFs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class FirstTransformer(Transformer):\n",
    "    def transform(self, inputDFs):\n",
    "        transaction_input_df = inputDFs.get(\"transaction_input_df\")\n",
    "        customer_input_df = inputDFs.get(\"customer_input_df\")  # Ensure correct key name\n",
    "\n",
    "        if transaction_input_df is None or customer_input_df is None:\n",
    "            print(\"❌ Error: Required DataFrames not found in inputDFs\")\n",
    "            return None\n",
    "\n",
    "        print(\"Displaying transaction_input_df:\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        print(\"Displaying customer_input_df:\")\n",
    "        customer_input_df.show()\n",
    "\n",
    "        # ✅ Convert `customer_id` in transaction_input_df to match customer_input_df type (string)\n",
    "        transaction_input_df = transaction_input_df.withColumn(\"customer_id\", col(\"customer_id\").cast(\"string\"))\n",
    "\n",
    "        # Define Window Specification\n",
    "        windowSpec = Window.partitionBy(\"customer_id\").orderBy(\"transaction_date\")\n",
    "\n",
    "        # Apply lead function to get next product\n",
    "        transformedDF = transaction_input_df.withColumn(\n",
    "            \"next_product_name\", lead(\"product_name\").over(windowSpec)\n",
    "        )\n",
    "\n",
    "        print(\"🔍 Filtering: Customers who bought an iPhone and then AirPods\")\n",
    "        filteredDF = transformedDF.filter(\n",
    "            (transformedDF.product_name == \"iPhone\") & \n",
    "            (transformedDF.next_product_name == \"AirPods\")\n",
    "        )\n",
    "\n",
    "        # ✅ Join & Select Only Required Columns\n",
    "        finalDF = filteredDF.join(customer_input_df, \"customer_id\", \"inner\") \\\n",
    "                            .select(\"customer_id\", \"customer_name\", \"location\")\n",
    "\n",
    "        # Show results\n",
    "        finalDF.show()\n",
    "\n",
    "        print(\"✅ Final Processed Data:\")\n",
    "        return finalDF  # Returning cleaned DataFrame\n",
    "\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755410de-ae03-49aa-b56f-416834ac5dd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Transformation complete.\nOut[19]: {'transaction_input_df': DataFrame[transaction_id: int, customer_id: int, product_name: string, transaction_date: date],\n 'customer_input_df': DataFrame[customer_id: string, customer_name: string, join_date: timestamp, location: string]}"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        )\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        customer_input_df = None  # Initialize in case of error\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\": transaction_input_df,\n",
    "            \"customer_input_df\": customer_input_df,\n",
    "        }\n",
    "\n",
    "        return input_dfs  # Ensure this is returned if needed\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8486d5b-dbdf-4a8b-839f-4caeeec8e6a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Transformation complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3afa1d9d-4945-45ea-83cf-0e56c827aefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n+-----------+-------------+-------------------+--------+\n|customer_id|customer_name|          join_date|location|\n+-----------+-------------+-------------------+--------+\n|        105|          Eva|2022-01-01 00:00:00|    Ohio|\n|        106|        Frank|2022-02-01 00:00:00|  Nevada|\n|        107|        Grace|2022-03-01 00:00:00|Colorado|\n|        108|        Henry|2022-04-01 00:00:00|    Utah|\n+-----------+-------------+-------------------+--------+\n\n✅ Transformation complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        try:\n",
    "            # Load Customer Data from Delta Table\n",
    "            customer_input_df = self.spark.read.format(\"delta\").table(\"default.customer_delta_table\")\n",
    "\n",
    "            if customer_input_df.count() > 0:\n",
    "                customer_input_df.show()\n",
    "                print(\"✅ Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "        \n",
    "        input_dfs = {\n",
    "            \"transaction_input_df\":transaction_input_df,\n",
    "            \"customer_input_df\":customer_input_df\n",
    "        }\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b930dc9a-ac8b-4360-9e8d-34d12570edb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"the bigdatashow.me\").getOrCreate()\n",
    "\n",
    "\n",
    "input_df = spark.read.format(\"csv\").option(\"header\", True).load(\"dbfs:/FileStore/tables/Transaction_Updated.csv\")\n",
    "\n",
    "input_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e443cea-6f4c-4622-9bea-2aa11ae12d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataSourceExample\").getOrCreate()\n",
    "\n",
    "class DataSource:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def get_dataframe(self):\n",
    "        raise ValueError(\"Not implemented\")\n",
    "\n",
    "class CSVDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        return spark.read.format(\"csv\").option(\"header\", True).load(self.path)\n",
    "\n",
    "class ParquetDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        return spark.read.format(\"parquet\").load(self.path)\n",
    " \n",
    "class DeltaDataSource(DataSource):\n",
    "    def get_dataframe(self):\n",
    "        return spark.read.format(\"delta\").load(self.path)\n",
    "\n",
    "def get_data_source(data_type, file_path):\n",
    "    if data_type == \"csv\":\n",
    "        return CSVDataSource(file_path)\n",
    "    elif data_type == \"parquet\":\n",
    "        return ParquetDataSource(file_path)\n",
    "    elif data_type == \"delta\":\n",
    "        return DeltaDataSource(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Not implemented for data_type: {data_type}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_source = get_data_source(\"csv\", \"dbfs:/FileStore/tables/Transaction_Updated.csv\")\n",
    "    df = data_source.get_dataframe()\n",
    "    df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d3074c-c464-4a47-b45c-c54320492bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Reader_Factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd03119-0d50-42f7-a225-4b50d54b9b19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        \n",
    "        transaction_input_df = get_data_source(data_type=\"csv\", file_path=\"dbfs:/FileStore/tables/Transaction_Updated.csv\").get_dataframe()\n",
    "\n",
    "       \n",
    "        transaction_input_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29a5b6c-5df6-43d2-91c4-31f0c48bae6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"./Reader_Factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45048486-1dca-4dd6-a8e6-7fdfdfb90941",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        \n",
    "        transaction_input_df = get_data_source(data_type=\"csv\", file_path=\"dbfs:/FileStore/tables/Transaction_Updated.csv\").get_dataframe()\n",
    "        transaction_input_df.show()\n",
    "\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7568664-6412-4b93-add4-e683622eb6c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def runner(self):\n",
    "        \n",
    "        transaction_input_df = get_data_source(data_type=\"csv\", file_path=\"dbfs:/FileStore/tables/Transaction_Updated.csv\").get_dataframe()\n",
    "        transaction_input_df.orderBy(\"customer_id,transaction_date\").show()\n",
    "        inputDFs = {\n",
    "            \"transaction_input_df\":transaction_input_df\n",
    "        }\n",
    "        firstTransform= FirstTransformer().transform\n",
    "        inputDFs\n",
    "\n",
    "workflow = Workflow()\n",
    "workflow.runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed2df296-46d6-4ca0-ba16-eb17086ed394",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Transform\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46bc7808-7d7a-4854-af26-63d78fbfab9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying transaction_input_df in transform:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nTransformation complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        transaction_input_df = self.spark.read.csv(\"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True)\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        inputDFs = {\"transaction_input_df\": transaction_input_df}\n",
    "\n",
    "        # Use FirstTransformer from the imported notebook\n",
    "        firstTransform = FirstTransformer().transform\n",
    "        transformed_df = firstTransform(inputDFs)\n",
    "\n",
    "        if transformed_df:\n",
    "            print(\"Transformation complete.\")\n",
    "\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "826f8916-2e34-4a2b-b0d5-df3984733247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21593ee2-4c87-4398-a09d-73214d092e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying transaction_input_df in transform:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nAirPods after buying iPhone\n+--------------+-----------+------------+----------------+-----------------+\n|transaction_id|customer_id|product_name|transaction_date|next_product_name|\n+--------------+-----------+------------+----------------+-----------------+\n|            11|        105|      iPhone|      2022-02-01|          AirPods|\n|            14|        105|     AirPods|      2022-02-04|          MacBook|\n|            18|        105|     MacBook|      2022-02-08|             null|\n|            12|        106|      iPhone|      2022-02-02|          MacBook|\n|            16|        106|     MacBook|      2022-02-06|          AirPods|\n|            20|        106|     AirPods|      2022-02-10|             null|\n|            13|        107|     AirPods|      2022-02-03|           iPhone|\n|            17|        107|      iPhone|      2022-02-07|             null|\n|            15|        108|      iPhone|      2022-02-05|          AirPods|\n|            19|        108|     AirPods|      2022-02-09|             null|\n+--------------+-----------+------------+----------------+-----------------+\n\nTransformation complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        transaction_input_df = self.spark.read.csv(\"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True)\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        inputDFs = {\"transaction_input_df\": transaction_input_df}\n",
    "\n",
    "        # Use FirstTransformer from the imported notebook\n",
    "        firstTransform = FirstTransformer().transform\n",
    "        transformed_df = firstTransform(inputDFs)\n",
    "\n",
    "        if transformed_df:\n",
    "            print(\"Transformation complete.\")\n",
    "\n",
    "workflow = Workflow()\n",
    "workflow.runner() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83418d0-6395-4e24-b175-00322696bd90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Transform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53797d9f-b964-4d12-bcd6-3f3a7826aa0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nDisplaying transaction_input_df in transform:\n+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\nAirPods after buying iPhone\n+--------------+-----------+------------+----------------+-----------------+\n|transaction_id|customer_id|product_name|transaction_date|next_product_name|\n+--------------+-----------+------------+----------------+-----------------+\n|            11|        105|      iPhone|      2022-02-01|          AirPods|\n|            14|        105|     AirPods|      2022-02-04|          MacBook|\n|            18|        105|     MacBook|      2022-02-08|             null|\n|            12|        106|      iPhone|      2022-02-02|          MacBook|\n|            16|        106|     MacBook|      2022-02-06|          AirPods|\n|            20|        106|     AirPods|      2022-02-10|             null|\n|            13|        107|     AirPods|      2022-02-03|           iPhone|\n|            17|        107|      iPhone|      2022-02-07|             null|\n|            15|        108|      iPhone|      2022-02-05|          AirPods|\n|            19|        108|     AirPods|      2022-02-09|             null|\n+--------------+-----------+------------+----------------+-----------------+\n\nFiltering: Customers who bought an iPhone and then AirPods\n+--------------+-----------+------------+----------------+-----------------+\n|transaction_id|customer_id|product_name|transaction_date|next_product_name|\n+--------------+-----------+------------+----------------+-----------------+\n|            11|        105|      iPhone|      2022-02-01|          AirPods|\n|            15|        108|      iPhone|      2022-02-05|          AirPods|\n+--------------+-----------+------------+----------------+-----------------+\n\nTransformation complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        transaction_input_df = self.spark.read.csv(\"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True)\n",
    "        transaction_input_df = transaction_input_df.orderBy(\"customer_id\", \"transaction_date\")\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        inputDFs = {\"transaction_input_df\": transaction_input_df}\n",
    "\n",
    "        # Use FirstTransformer from the imported notebook\n",
    "        firstTransform = FirstTransformer().transform\n",
    "        transformed_df = firstTransform(inputDFs)\n",
    "\n",
    "        if transformed_df:\n",
    "            print(\"Transformation complete.\")\n",
    "\n",
    "workflow = Workflow()\n",
    "workflow.runner() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d7a7a2-67eb-4887-8c29-bc257c62a063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Reader_Factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe57a9d-fd45-4e52-b1ca-9dc3c149eb64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            12|        106|      iPhone|      2022-02-02|\n|            13|        107|     AirPods|      2022-02-03|\n|            14|        105|     AirPods|      2022-02-04|\n|            15|        108|      iPhone|      2022-02-05|\n|            16|        106|     MacBook|      2022-02-06|\n|            17|        107|      iPhone|      2022-02-07|\n|            18|        105|     MacBook|      2022-02-08|\n|            19|        108|     AirPods|      2022-02-09|\n|            20|        106|     AirPods|      2022-02-10|\n+--------------+-----------+------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Reader_Factory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4526c4b0-1706-4446-b478-88745015e820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3834799890448289>:3\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Import get_data_source from your DataSource script\u001B[39;00m\n",
       "\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasource\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_data_source  \u001B[38;5;66;03m# If using a Python file\u001B[39;00m\n",
       "\u001B[1;32m      4\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrun\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Reader_Factory\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mWorkflow\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n",
       "\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n",
       "\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n",
       "\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
       "\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n",
       "\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'datasource'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)\nFile \u001B[0;32m<command-3834799890448289>:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SparkSession\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# Import get_data_source from your DataSource script\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasource\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_data_source  \u001B[38;5;66;03m# If using a Python file\u001B[39;00m\n\u001B[1;32m      4\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_line_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrun\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Reader_Factory\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mWorkflow\u001B[39;00m:\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/PythonPackageImportsInstrumentation/__init__.py:171\u001B[0m, in \u001B[0;36m_create_import_patch.<locals>.import_patch\u001B[0;34m(name, globals, locals, fromlist, level)\u001B[0m\n\u001B[1;32m    166\u001B[0m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# Import the desired module. If you’re seeing this while debugging a failed import,\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     \u001B[38;5;66;03m# look at preceding stack frames for relevant error information.\u001B[39;00m\n\u001B[0;32m--> 171\u001B[0m     original_result \u001B[38;5;241m=\u001B[39m \u001B[43mpython_builtin_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mglobals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlocals\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfromlist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    173\u001B[0m     is_root_import \u001B[38;5;241m=\u001B[39m thread_local\u001B[38;5;241m.\u001B[39m_nest_level \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    174\u001B[0m     \u001B[38;5;66;03m# `level` represents the number of leading dots in a relative import statement.\u001B[39;00m\n\u001B[1;32m    175\u001B[0m     \u001B[38;5;66;03m# If it's zero, then this is an absolute import.\u001B[39;00m\n\n\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'datasource'",
       "errorSummary": "<span class='ansi-red-fg'>ModuleNotFoundError</span>: No module named 'datasource'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Import get_data_source from your DataSource script\n",
    "from datasource import get_data_source  # If using a Python file\n",
    "%run \"/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis/Reader_Factory\"\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        # Singleton SparkSession\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        # Load Customer Data from Delta Table\n",
    "        try:\n",
    "            customer_input_df = get_data_source(\n",
    "                data_type=\"delta\",\n",
    "                file_path=\"default.customer_delta_table\"\n",
    "            ).get_dataframe()\n",
    "\n",
    "            if customer_input_df is not None:\n",
    "                customer_input_df.show()  # Display customer data\n",
    "                print(\"Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d772e87e-f652-420f-98a3-8f3901d2a64c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n❌ Error loading customer data: name 'get_data_source' is not defined\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Workspace/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis')\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        # Load Customer Data from Delta Table\n",
    "        try:\n",
    "            customer_input_df = get_data_source(\n",
    "                data_type=\"delta\",\n",
    "                file_path=\"default.customer_delta_table\"\n",
    "            ).get_dataframe()\n",
    "\n",
    "            if customer_input_df is not None:\n",
    "                customer_input_df.show()  # Display customer data\n",
    "                print(\"Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fc80f4-b193-48a4-86fa-854ac8a85c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n❌ Error loading customer data: name 'get_data_source' is not defined\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Workspace/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis')\n",
    "\n",
    "from pyspark.sql import SparkSession              \n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        # Load Customer Data from Delta Table (Corrected)\n",
    "        try:\n",
    "            customer_input_df = get_data_source(\n",
    "                data_type=\"delta\",\n",
    "                file_path=\"default.customer_delta_table\"\n",
    "            ).get_dataframe()\n",
    "\n",
    "            if customer_input_df is not None and customer_input_df.count() > 0:\n",
    "                customer_input_df.show()  # Display customer data\n",
    "                print(\"✅ Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348c434f-fec8-4a2d-be8c-9d66c703b4ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+------------+----------------+\n|transaction_id|customer_id|product_name|transaction_date|\n+--------------+-----------+------------+----------------+\n|            11|        105|      iPhone|      2022-02-01|\n|            14|        105|     AirPods|      2022-02-04|\n|            18|        105|     MacBook|      2022-02-08|\n|            12|        106|      iPhone|      2022-02-02|\n|            16|        106|     MacBook|      2022-02-06|\n|            20|        106|     AirPods|      2022-02-10|\n|            13|        107|     AirPods|      2022-02-03|\n|            17|        107|      iPhone|      2022-02-07|\n|            15|        108|      iPhone|      2022-02-05|\n|            19|        108|     AirPods|      2022-02-09|\n+--------------+-----------+------------+----------------+\n\n❌ Error loading customer data: name 'get_data_source' is not defined\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Workspace/Users/vakhiliny@std.appsc.sab.ac.lk/Apple_Analysis')\n",
    "\n",
    "from pyspark.sql import SparkSession              \n",
    "\n",
    "class Workflow:\n",
    "    def __init__(self):\n",
    "        self.spark = SparkSession.builder.appName(\"Workflow\").getOrCreate()\n",
    "\n",
    "    def runner(self):\n",
    "        # Load Transaction Data from CSV\n",
    "        transaction_input_df = self.spark.read.csv(\n",
    "            \"dbfs:/FileStore/tables/Transaction_Updated.csv\", header=True, inferSchema=True\n",
    "        ).orderBy(\"customer_id\", \"transaction_date\")\n",
    "\n",
    "        transaction_input_df.show()\n",
    "\n",
    "        try:\n",
    "            customer_input_df = get_data_source(\n",
    "                data_type=\"delta\",\n",
    "                file_path=\"default.customer_delta_table\"\n",
    "            ).get_dataframe()\n",
    "\n",
    "            if customer_input_df is not None and customer_input_df.count() > 0:\n",
    "                customer_input_df.show()  # Display customer data\n",
    "                print(\"✅ Transformation complete.\")\n",
    "            else:\n",
    "                print(\"⚠️ Warning: No data found in Delta table!\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading customer data: {e}\")\n",
    "\n",
    "# Run Workflow\n",
    "workflow = Workflow()\n",
    "workflow.runner()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Apple_Analysis",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
